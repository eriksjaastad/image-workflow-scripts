# Meta-Prompt Raptor Review

_Date:_ 2025-11-01 00:26:27Z UTC

## Phase A ‚Äì Claude Sonnet 4.5 (Max Mode)

Prompt file: /Users/eriksjaastad/projects/image-workflow/prompts/raptor_phase_a_sonnet.md

# RAPTOR Phase A Reliability Review

**Target:** `scripts/03_web_character_sorter.py`  
**Reviewer:** Claude Sonnet 4.5 (Phase A - Fragility Analysis)  
**Date:** 2025-11-01T00:28:39Z  
**Project:** image-workflow-scripts (Python 3.11)

---

## Executive Summary

This web-based character sorting tool shows **moderate to high fragility risk** due to **10 silent exception handlers**, several **best-effort patterns that swallow errors**, and **missing crash recovery**. While the script uses FileTracker for audit trails, it has **no structured logging**, and multiple failure points could result in **silent data loss** or **incomplete operations** that would go undetected until manual inspection.

**Critical Risk:** The script processes user decisions on thousands of images but has no heartbeat verification or operation confirmation system. A silent failure in file operations would not trigger alerts.

---

## === FINDINGS ===

### üö® CRITICAL FRAGILITY ZONES

#### 1. **Silent Exception Handlers (10 instances)**

**Severity:** HIGH - Violates CODE_QUALITY_RULES.md Rule #1

Multiple `except Exception` blocks that either:

- Print a message and continue (swallowing the error)
- Pass silently without any indication of failure
- Don't re-raise, allowing corrupt state to propagate

**Locations:**

- Line 118-120: Flask import guard (empty try block - suspicious)
- Line 122-126: Pillow import guard (empty try block with raise)
- Line 133-134: HEIC support (silent pass)
- Line 139-140: send2trash availability check (silent pass)
- Line 220-221: Progress save error (prints, continues)
- Line 340-341: Progress cleanup error (prints, continues)
- Line 377-378: Similarity map load (prints, continues)
- Line 628-630: Similarity map cleanup (prints, returns False)
- Line 908-909: File move operation (returns 500 error)
- Line 935-936: File delete operation (returns 500 error)
- Line 1067-1070: Companion file move (returns 500 error)
- Line 1082-1083: **MOST DANGEROUS** - Best-effort logging with `except Exception: pass`
- Line 1154-1155: Batch processing (returns 500 error)

**Impact:**

- File operations can fail silently
- Progress tracking can break without warning
- User thinks operation succeeded but files weren't moved
- No audit trail when exceptions occur

#### 2. **Best-Effort Antipattern (Line 1082-1083)**

**Severity:** CRITICAL

```python
except Exception:
    pass  # best-effort logging
```

This is the **most dangerous pattern** in the codebase. The comment explicitly acknowledges that logging failures are ignored. If FileTracker operations fail, there's **no audit trail** for file operations, defeating the entire purpose of tracking.

**Production Scenario:**

1. User processes 500 images over 2 hours
2. FileTracker log_operation() fails silently at image #200
3. Remaining 300 operations have no audit trail
4. User has no idea that tracking stopped
5. Manual recovery impossible without logs

#### 3. **Empty Try Block (Line 122-124)**

**Severity:** HIGH

```python
try:
    pass
except Exception:
    print("[!] Pillow is required. Install with: pip install pillow", file=sys.stderr)
    raise
```

This is **dead code** - the try block does nothing. This suggests either:

- Incomplete refactoring
- Lost import statement during editing
- Copy-paste error

The code will never fail here because `pass` never raises an exception.

#### 4. **Progress Tracking Fragility**

**Severity:** MEDIUM-HIGH

The `MultiDirectoryProgressTracker` class has multiple error-prone patterns:

**Line 217-221:** Progress save failures are logged to console but don't halt execution:

```python
try:
    with open(self.progress_file, "w") as f:
        json.dump(self.session_data, f, indent=2)
except Exception as e:
    print(f"[!] Error saving progress: {e}")
```

**Problems:**

- Disk full? Silent failure, progress lost
- Permission denied? Silent failure, progress lost
- JSON serialization error? Silent failure, progress lost
- User has no indication their progress isn't being saved

**Production Impact:**

- User processes directories 1-10
- Disk fills up at directory 5
- Progress saving silently fails
- User closes app thinking progress is saved
- Next run: starts from directory 1 again
- Hours of work lost

#### 5. **Similarity Map Operations (No Failure Validation)**

**Severity:** MEDIUM

**Line 377-380:** Loading similarity maps fails silently:

```python
except Exception as e:
    info(f"Failed to load similarity map: {e}")
    return neighbors
```

Returns an empty dictionary, causing the UI to fall back to alphabetical sorting. User expects similarity-based layout but gets different results with no clear indication why.

**Line 628-630:** Cleaning similarity maps fails silently:

```python
except Exception as e:
    info(f"Failed to clean similarity maps: {e}")
    return False
```

Returns False, but caller at line 1025-1026 doesn't check the return value:

```python
if similarity_map_dir:
    clean_similarity_maps(app.config["CURRENT_DIR"], similarity_map_dir)
```

Result: Stale similarity data persists, causing deleted images to appear in neighbor calculations.

#### 6. **No Structured Logging**

**Severity:** MEDIUM

The script uses `print()` statements throughout (which is allowed for CLI scripts per CODE_QUALITY_RULES.md), but this means:

- No log levels (can't filter INFO vs ERROR)
- No timestamps on messages
- Can't redirect to file for post-mortem analysis
- No structured data for parsing/alerting

For a tool that processes thousands of images, **no persistent error log** means debugging production failures requires user memory or FileTracker reconstruction.

#### 7. **File Operation Error Handling**

**Severity:** MEDIUM-HIGH

Multiple Flask endpoints catch exceptions and return HTTP 500 but don't log the failure:

**Line 908-909:**

```python
except Exception as e:
    return jsonify({"status": "error", "message": str(e)}), 500
```

**Problems:**

- Exception message goes to user's browser
- No server-side logging of the error
- No stack trace captured
- Can't debug issues reported by users

#### 8. **Signal Handler Not Registered**

**Severity:** MEDIUM

The script imports `signal` (line 90) but never registers signal handlers. This means:

- CTRL+C might leave Flask server in inconsistent state
- No graceful shutdown cleanup
- Progress files might be mid-write when interrupted
- No opportunity to flush FileTracker logs

#### 9. **Import Guards Without Logging**

**Severity:** LOW-MEDIUM

**Line 133-134, 139-140:** Import failures pass silently:

```python
except Exception:
    pass
```

For HEIC support and send2trash, this is acceptable (they're optional dependencies). However, the script doesn't log that these features are disabled. User might wonder why HEIC files don't work.

#### 10. **No Operation Confirmation**

**Severity:** MEDIUM

After batch operations, there's no verification that files actually moved/deleted:

- `move_with_metadata()` returns list of moved files but not all callers check
- `safe_delete()` delegates to utility function with no success confirmation
- No checksum verification that moves completed correctly
- No file existence check after operations

---

## === DIFFS (MAX 5) ===

### DIFF 1: Fix Critical Best-Effort Logging Antipattern

**Priority:** CRITICAL  
**Location:** Lines 1067-1083

```diff
--- a/scripts/03_web_character_sorter.py
+++ b/scripts/03_web_character_sorter.py
@@ -1064,21 +1064,27 @@ def create_app(
                 try:
                     move_file_with_all_companions(d, staging, dry_run=False)
                     moved.append(d.name)
                 except Exception as e:
-                    return jsonify(
-                        {"status": "error", "message": f"Failed moving {d.name}: {e}"}
-                    ), 500
+                    error_msg = f"Failed moving orphan .decision file {d.name}: {e}"
+                    print(f"[!] {error_msg}", file=sys.stderr)
+                    return jsonify({"status": "error", "message": error_msg}), 500

             if moved:
-                # Best-effort logging of .decision cleanup
-                try:
-                    tracker.log_operation(
-                        operation="move",
-                        source_dir=str(folder),
-                        dest_dir="__delete_staging",
-                        file_count=len(moved),
-                        files=moved,
-                        notes=f"orphan .decision staged ({len(moved)} total)",
-                    )
-                except Exception:
-                    pass  # best-effort logging
+                # Log .decision cleanup - critical for audit trail
+                log_msg = f"orphan .decision staged ({len(moved)} total)"
+                try:
+                    tracker.log_operation(
+                        operation="move",
+                        source_dir=str(folder),
+                        dest_dir="__delete_staging",
+                        file_count=len(moved),
+                        files=moved,
+                        notes=log_msg,
+                    )
+                except Exception as e:
+                    # Logging failure is CRITICAL - don't silently ignore
+                    error_msg = f"CRITICAL: FileTracker logging failed for .decision cleanup: {e}"
+                    print(f"[!!!] {error_msg}", file=sys.stderr)
+                    # Don't fail the request, but user MUST know logging is broken
+                    print(f"[!!!] Audit trail incomplete! Manual verification required for: {moved}", file=sys.stderr)

         return jsonify(
```

**Rationale:**

- Removes the most dangerous silent exception handler
- Adds stderr logging for all error cases
- Makes it obvious when audit trail is compromised
- User gets clear warning that manual verification is needed

---

### DIFF 2: Add Structured Logging for Production Debugging

**Priority:** HIGH  
**Location:** Top of file + critical error handlers

```diff
--- a/scripts/03_web_character_sorter.py
+++ b/scripts/03_web_character_sorter.py
@@ -85,6 +85,7 @@ from __future__ import annotations

 import argparse
 import json
+import logging
 import os
 import signal
 import sys
@@ -95,6 +96,15 @@ from pathlib import Path
 from typing import Dict, List, Optional

 from file_tracker import FileTracker
+
+# Configure logging for production debugging
+logging.basicConfig(
+    level=logging.INFO,
+    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
+    handlers=[
+        logging.StreamHandler(sys.stdout)
+    ]
+)
+logger = logging.getLogger(__name__)

 from utils.companion_file_utils import (
     format_image_display_name,
@@ -217,7 +227,8 @@ class MultiDirectoryProgressTracker:
         try:
             with open(self.progress_file, "w") as f:
                 json.dump(self.session_data, f, indent=2)
         except Exception as e:
-            print(f"[!] Error saving progress: {e}")
+            logger.error(f"Failed to save progress to {self.progress_file}: {e}", exc_info=True)
+            raise  # Don't continue with broken progress tracking

     def get_current_directory(self) -> Optional[Dict]:
@@ -337,7 +348,8 @@ class MultiDirectoryProgressTracker:
             if self.progress_file.exists():
                 self.progress_file.unlink()
-                print(f"[*] Cleaned up progress file: {self.progress_file}")
+                logger.info(f"Cleaned up progress file: {self.progress_file}")
         except Exception as e:
-            print(f"[!] Error cleaning up progress file: {e}")
+            logger.error(f"Failed to cleanup progress file {self.progress_file}: {e}", exc_info=True)
+            # Non-critical - don't raise
```

**Rationale:**

- Adds structured logging with timestamps and log levels
- Stack traces via `exc_info=True` for debugging
- Progress save failures now raise (fail-fast instead of silent corruption)
- Cleanup failures are logged but don't raise (non-critical)
- All logs go to stdout (can be redirected to file)

---

### DIFF 3: Fix Empty Try Block and Import Guards

**Priority:** HIGH  
**Location:** Lines 122-140

```diff
--- a/scripts/03_web_character_sorter.py
+++ b/scripts/03_web_character_sorter.py
@@ -119,19 +119,12 @@ try:
     )
-except Exception:  # pragma: no cover - import guard for clearer error
+except ImportError as e:  # pragma: no cover - import guard for clearer error
     print("[!] Flask is required. Install with: pip install flask", file=sys.stderr)
-    raise
-
-try:
-    pass
-except Exception:
-    print("[!] Pillow is required. Install with: pip install pillow", file=sys.stderr)
+    logger.critical("Flask import failed - cannot run web interface", exc_info=True)
     raise

 try:
     from pillow_heif import register_heif_opener  # type: ignore
-
     register_heif_opener()
-    print("[*] HEIC/HEIF support enabled via pillow-heif.")
-except Exception:
-    pass
+    logger.info("HEIC/HEIF support enabled via pillow-heif")
+except ImportError:
+    logger.info("pillow-heif not available - HEIC/HEIF files will not be supported")
+except Exception as e:
+    logger.warning(f"pillow-heif registration failed: {e} - HEIC/HEIF support disabled")

 _SEND2TRASH_AVAILABLE = False
 try:
+    from send2trash import send2trash
     _SEND2TRASH_AVAILABLE = True
-except Exception:
+    logger.info("send2trash available - safe deletion enabled")
+except ImportError:
     _SEND2TRASH_AVAILABLE = False
+    logger.warning("send2trash not available - will use hard delete (--hard-delete behavior)")
```

**Rationale:**

- Removes empty try block (dead code)
- Removes redundant Pillow import guard
- Catches `ImportError` specifically instead of broad `Exception`
- Logs optional dependency status so users understand feature availability
- Uses structured logging for better debugging

---

### DIFF 4: Add Graceful Shutdown Handler

**Priority:** MEDIUM  
**Location:** After imports + main()

```diff
--- a/scripts/03_web_character_sorter.py
+++ b/scripts/03_web_character_sorter.py
@@ -142,6 +142,29 @@ _SEND2TRASH_AVAILABLE = False

 THUMBNAIL_MAX_DIM = 800

+# Global shutdown flag for graceful cleanup
+_shutdown_requested = False
+
+def _signal_handler(signum, frame):
+    """Handle SIGINT/SIGTERM for graceful shutdown."""
+    global _shutdown_requested
+    if _shutdown_requested:
+        logger.warning("Forced shutdown - progress may be lost")
+        sys.exit(1)
+
+    _shutdown_requested = True
+    signal_name = signal.Signals(signum).name
+    logger.info(f"Received {signal_name} - shutting down gracefully...")
+    print("\n[*] Shutting down gracefully - please wait for cleanup...")
+
+    # Flask will shut down on its own when the handler returns
+    # Progress files are saved throughout operation, so no final save needed
+    print("[*] Shutdown complete. Progress has been saved.")
+    sys.exit(0)
+
+# Register signal handlers for graceful shutdown
+signal.signal(signal.SIGINT, _signal_handler)
+signal.signal(signal.SIGTERM, _signal_handler)
+

 class MultiDirectoryProgressTracker:
     """Manages progress tracking across multiple directories with session persistence."""
@@ -3160,8 +3183,13 @@ def main():

     try:
         info(f"Character sorter running at http://{args.host}:{args.port}")
+        logger.info(f"Starting Flask server on {args.host}:{args.port}")
         app.run(host=args.host, port=args.port, debug=False, use_reloader=False)
     except OSError as exc:
-        human_err(f"Failed to start server: {exc}")
+        logger.critical(f"Failed to start Flask server on {args.host}:{args.port}: {exc}", exc_info=True)
+        human_err(f"Failed to start server: {exc}")
+        sys.exit(1)
+    except KeyboardInterrupt:
+        # Handled by signal handler
         sys.exit(1)
```

**Rationale:**

- Registers SIGINT (Ctrl+C) and SIGTERM handlers
- Allows graceful shutdown instead of abrupt termination
- Prevents interrupted JSON writes to progress files
- Gives user clear feedback about shutdown process
- Double Ctrl+C forces immediate exit (safety valve)

---

### DIFF 5: Add File Operation Verification

**Priority:** MEDIUM  
**Location:** Lines 530-550 (move_with_metadata function)

```diff
--- a/scripts/03_web_character_sorter.py
+++ b/scripts/03_web_character_sorter.py
@@ -530,6 +530,7 @@ def move_with_metadata(
     src_path: Path, dest_dir: Path, tracker: FileTracker, group_name: str
 ) -> List[str]:
     """Move PNG and ALL corresponding companion files to destination directory."""
+    # Pre-check: source must exist
+    if not src_path.exists():
+        error_msg = f"Source file does not exist: {src_path}"
+        logger.error(error_msg)
+        raise FileNotFoundError(error_msg)
+
     dest_dir.mkdir(exist_ok=True)

     # Use wildcard logic to move PNG and ALL companion files
     moved_files = move_file_with_all_companions(src_path, dest_dir, dry_run=False)
+
+    # Post-check: verify files actually moved
+    if not moved_files:
+        error_msg = f"No files moved for {src_path.name} - operation may have failed silently"
+        logger.error(error_msg)
+        raise RuntimeError(error_msg)
+
+    # Verify primary file exists at destination
+    dest_file = dest_dir / src_path.name
+    if not dest_file.exists():
+        error_msg = f"Move reported success but destination file missing: {dest_file}"
+        logger.error(error_msg)
+        raise RuntimeError(error_msg)
+
+    logger.info(f"Successfully moved {src_path.name} + {len(moved_files)-1} companions to {dest_dir}")

     # Log the operation
     tracker.log_operation(
         operation="move",
         source_dir=src_path.parent.name,
         dest_dir=group_name,
         file_count=len(moved_files),
         files=moved_files,
         notes=f"User selected {group_name}",
     )
```

**Rationale:**

- Adds pre-check: source file must exist before attempting move
- Adds post-check: verify files actually moved to destination
- Verifies primary file exists at destination
- Raises exceptions on verification failure (fail-fast)
- Logs successful operations for debugging
- Prevents silent "success" when operation actually failed

---

## === TESTS ===

### Test Suite for Reliability Fixes

```python
"""
test_03_web_character_sorter_reliability.py

Test suite targeting fragility zones identified in RAPTOR Phase A review.
Run with: pytest -xvs test_03_web_character_sorter_reliability.py
"""

import json
import tempfile
from pathlib import Path
from unittest.mock import Mock, patch, MagicMock
import pytest

# Import the module under test
import sys
sys.path.insert(0, str(Path(__file__).parent.parent / "scripts"))

from scripts.03_web_character_sorter import (
    MultiDirectoryProgressTracker,
    move_with_metadata,
    clean_similarity_maps,
    load_similarity_neighbors,
)
from file_tracker import FileTracker


class TestProgressTrackingReliability:
    """Test fragility zone: Progress tracking must fail-fast on errors."""

    def test_progress_save_failure_raises_exception(self, tmp_path):
        """Progress save errors must raise, not silently fail."""
        tracker = MultiDirectoryProgressTracker(tmp_path)

        # Make progress file read-only to trigger save failure
        tracker.progress_file.touch()
        tracker.progress_file.chmod(0o444)

        # Progress save should raise, not silently fail
        with pytest.raises(Exception) as exc_info:
            tracker.save_progress()

        # Verify error message is informative
        assert "progress" in str(exc_info.value).lower()

    def test_progress_save_disk_full_scenario(self, tmp_path, monkeypatch):
        """Simulate disk full during progress save - must raise."""
        tracker = MultiDirectoryProgressTracker(tmp_path)

        # Mock json.dump to raise IOError (disk full)
        def mock_dump(*args, **kwargs):
            raise IOError("No space left on device")

        monkeypatch.setattr("json.dump", mock_dump)

        # Should raise, not silently fail
        with pytest.raises(IOError) as exc_info:
            tracker.save_progress()

        assert "space" in str(exc_info.value).lower()


class TestFileOperationVerification:
    """Test fragility zone: File operations must be verified."""

    def test_move_with_nonexistent_source_raises(self, tmp_path):
        """Moving nonexistent file must raise FileNotFoundError."""
        dest_dir = tmp_path / "dest"
        dest_dir.mkdir()

        nonexistent = tmp_path / "nonexistent.png"
        tracker = FileTracker("test")

        # Should raise immediately, not attempt to move
        with pytest.raises(FileNotFoundError) as exc_info:
            move_with_metadata(nonexistent, dest_dir, tracker, "test_group")

        assert "does not exist" in str(exc_info.value).lower()

    def test_move_verifies_destination_exists(self, tmp_path, monkeypatch):
        """Move operation must verify file exists at destination."""
        src = tmp_path / "src.png"
        src.write_text("test")

        dest_dir = tmp_path / "dest"
        dest_dir.mkdir()

        tracker = FileTracker("test")

        # Mock move_file_with_all_companions to return success but not actually move
        def mock_move(*args, **kwargs):
            return ["src.png"]  # Claims success but doesn't move file

        monkeypatch.setattr(
            "scripts.03_web_character_sorter.move_file_with_all_companions",
            mock_move
        )

        # Should detect missing destination file and raise
        with pytest.raises(RuntimeError) as exc_info:
            move_with_metadata(src, dest_dir, tracker, "test_group")

        assert "destination file missing" in str(exc_info.value).lower()

    def test_move_with_no_files_moved_raises(self, tmp_path, monkeypatch):
        """Move operation returning empty list must raise."""
        src = tmp_path / "src.png"
        src.write_text("test")

        dest_dir = tmp_path / "dest"
        dest_dir.mkdir()

        tracker = FileTracker("test")

        # Mock move to return empty list (silent failure)
        monkeypatch.setattr(
            "scripts.03_web_character_sorter.move_file_with_all_companions",
            lambda *args, **kwargs: []
        )

        # Should raise, not silently accept empty result
        with pytest.raises(RuntimeError) as exc_info:
            move_with_metadata(src, dest_dir, tracker, "test_group")

        assert "no files moved" in str(exc_info.value).lower()


class TestSimilarityMapReliability:
    """Test fragility zone: Similarity map operations must handle errors properly."""

    def test_load_similarity_neighbors_corrupted_json(self, tmp_path):
        """Corrupted JSON in similarity map must be handled gracefully."""
        neighbors_file = tmp_path / "neighbors.jsonl"
        neighbors_file.write_text("not valid json\n")

        # Should log error but not crash
        result = load_similarity_neighbors(tmp_path)

        # Should return empty dict, not raise
        assert result == {}

    def test_clean_similarity_maps_missing_file(self, tmp_path):
        """Cleaning non-existent similarity map should return False."""
        folder = tmp_path / "folder"
        folder.mkdir()

        similarity_dir = tmp_path / "similarity"
        similarity_dir.mkdir()

        # No neighbors.jsonl exists
        result = clean_similarity_maps(folder, similarity_dir)

        # Should return False, not crash
        assert result is False

    def test_clean_similarity_maps_removes_deleted_images(self, tmp_path):
        """Cleaning similarity maps must remove references to deleted images."""
        folder = tmp_path / "folder"
        folder.mkdir()

        # Create some images
        (folder / "img1.png").touch()
        (folder / "img2.png").touch()
        # img3.png was deleted

        similarity_dir = tmp_path / "similarity"
        similarity_dir.mkdir()

        # Create neighbors.jsonl with reference to deleted image
        neighbors_file = similarity_dir / "neighbors.jsonl"
        neighbors_file.write_text(
            json.dumps({
                "filename": "img1.png",
                "neighbors": [
                    {"filename": "img2.png", "sim": 0.9},
                    {"filename": "img3.png", "sim": 0.8},  # Deleted
                ]
            }) + "\n" +
            json.dumps({
                "filename": "img3.png",  # Deleted
                "neighbors": [{"filename": "img1.png", "sim": 0.8}]
            }) + "\n"
        )

        result = clean_similarity_maps(folder, similarity_dir)
        assert result is True

        # Verify cleaned content
        cleaned = []
        with open(neighbors_file) as f:
            for line in f:
                cleaned.append(json.loads(line))

        # Should only have img1 entry, and img3 reference removed from neighbors
        assert len(cleaned) == 1
        assert cleaned[0]["filename"] == "img1.png"
        assert len(cleaned[0]["neighbors"]) == 1
        assert cleaned[0]["neighbors"][0]["filename"] == "img2.png"


class TestGracefulShutdown:
    """Test fragility zone: Shutdown must be graceful."""

    def test_signal_handler_sets_shutdown_flag(self, monkeypatch):
        """SIGINT/SIGTERM should set shutdown flag."""
        import signal
        from scripts.03_web_character_sorter import _signal_handler, _shutdown_requested

        # Capture the shutdown behavior
        with pytest.raises(SystemExit) as exc_info:
            _signal_handler(signal.SIGINT, None)

        assert exc_info.value.code == 0

    def test_double_signal_forces_immediate_exit(self, monkeypatch):
        """Second SIGINT should force immediate exit."""
        import signal
        from scripts.03_web_character_sorter import _signal_handler

        # First signal - graceful
        with pytest.raises(SystemExit) as exc_info:
            _signal_handler(signal.SIGINT, None)
        assert exc_info.value.code == 0

        # Second signal - forced
        with pytest.raises(SystemExit) as exc_info:
            _signal_handler(signal.SIGINT, None)
        assert exc_info.value.code == 1


class TestLoggingReliability:
    """Test fragility zone: Critical operations must be logged."""

    def test_file_tracker_failure_is_logged(self, tmp_path, monkeypatch, caplog):
        """FileTracker logging failure must be logged to stderr."""
        src = tmp_path / "src.png"
        src.write_text("test")

        dest_dir = tmp_path / "dest"
        dest_dir.mkdir()

        # Mock tracker to fail
        mock_tracker = Mock()
        mock_tracker.log_operation.side_effect = Exception("Logging failed")

        # Should log the error even though operation succeeds
        with pytest.raises(Exception):
            move_with_metadata(src, dest_dir, mock_tracker, "test_group")

        # Verify error was logged (not silently swallowed)
        # Note: With fixed code, this should log to stderr


# Parametrized tests for exception handling
@pytest.mark.parametrize("exception_type,expected_handling", [
    (FileNotFoundError, "should_raise"),
    (PermissionError, "should_raise"),
    (IOError, "should_raise"),
    (OSError, "should_raise"),
])
def test_file_operation_exception_handling(exception_type, expected_handling, tmp_path, monkeypatch):
    """File operations should handle specific exceptions, not broad Exception."""
    src = tmp_path / "src.png"
    src.write_text("test")

    dest_dir = tmp_path / "dest"
    dest_dir.mkdir()

    tracker = FileTracker("test")

    # Mock move to raise specific exception
    def mock_move(*args, **kwargs):
        raise exception_type("Test error")

    monkeypatch.setattr(
        "scripts.03_web_character_sorter.move_file_with_all_companions",
        mock_move
    )

    if expected_handling == "should_raise":
        with pytest.raises(exception_type):
            move_with_metadata(src, dest_dir, tracker, "test_group")
```

**Test Execution:**

```bash
# Run all reliability tests
pytest -xvs test_03_web_character_sorter_reliability.py

# Run specific test class
pytest -xvs test_03_web_character_sorter_reliability.py::TestProgressTrackingReliability

# Run with coverage
pytest --cov=scripts.03_web_character_sorter test_03_web_character_sorter_reliability.py
```

---

## === CHECKLIST ===

### Immediate Actions (Do Before Next Production Run)

- [ ] **CRITICAL:** Apply DIFF 1 - Fix best-effort logging antipattern

  - Remove `except Exception: pass` at line 1082-1083
  - Add explicit error logging for FileTracker failures
  - Test with intentionally broken FileTracker

- [ ] **HIGH:** Apply DIFF 2 - Add structured logging

  - Import logging module
  - Configure logging with timestamps
  - Replace critical print() with logger.error()
  - Test log output captures stack traces

- [ ] **HIGH:** Apply DIFF 3 - Fix empty try block and import guards

  - Remove dead code at line 122-124
  - Use specific exception types (ImportError, not Exception)
  - Log optional dependency availability
  - Test with missing dependencies

- [ ] **MEDIUM:** Apply DIFF 4 - Add graceful shutdown

  - Register SIGINT/SIGTERM handlers
  - Test Ctrl+C during operation
  - Verify progress files not corrupted on interrupt

- [ ] **MEDIUM:** Apply DIFF 5 - Add file operation verification
  - Add pre/post checks to move_with_metadata()
  - Test with nonexistent source files
  - Test with mocked move failures

### Testing & Validation

- [ ] **Create test suite:** Implement pytest suite from TESTS section

  - Test progress save failures raise exceptions
  - Test file operation verification catches failures
  - Test signal handlers work correctly
  - Aim for 80%+ coverage of error paths

- [ ] **Integration testing:** Test full workflow end-to-end

  - Process real directory of images
  - Trigger errors intentionally (disk full simulation)
  - Verify all errors are logged with timestamps
  - Verify FileTracker captures all operations

- [ ] **Failure injection testing:** Use chaos engineering
  - Inject OSError during file moves
  - Inject PermissionError during progress saves
  - Inject JSON parsing errors in similarity maps
  - Verify all failures are caught and logged

### CI/CD & Automation

- [ ] **Pre-commit hook:** Add to `.pre-commit-config.yaml`

  ```yaml
  - repo: local
    hooks:
      - id: check-silent-exceptions
        name: Check for silent exception handlers
        entry: python scripts/utils/check_silent_exceptions.py
        language: python
        files: \.py$
  ```

- [ ] **Ruff enforcement:** Verify Ruff catches broad exceptions

  ```bash
  ruff check scripts/03_web_character_sorter.py --select BLE,TRY
  ```

- [ ] **Logging verification:** Add CI check
  - Verify all `except Exception` have logging
  - Verify all file operations log success/failure
  - Run test suite in CI on every commit

### Documentation

- [ ] **Add OPERATIONS.md:** Document expected log output

  - Normal operation log patterns
  - Error scenarios and their log signatures
  - How to debug failures from logs

- [ ] **Add RECOVERY.md:** Document failure recovery procedures

  - How to recover from interrupted operations
  - How to verify FileTracker audit trail
  - How to manually reconcile moved files

- [ ] **Update README:** Add reliability section
  - Logging behavior (stdout vs stderr)
  - Expected error handling behavior
  - How to report issues with logs

### Monitoring & Observability

- [ ] **Add heartbeat logging:** Every N operations, log progress

  ```python
  if operations_count % 100 == 0:
      logger.info(f"Processed {operations_count} operations - still running")
  ```

- [ ] **Add operation summary:** At end of session, log stats

  ```python
  logger.info(f"Session complete: {moved} moved, {deleted} deleted, {errors} errors")
  ```

- [ ] **Add FileTracker verification:** After session, verify log integrity
  ```python
  # Verify all operations were logged
  logged_count = tracker.count_operations()
  if logged_count != expected_count:
      logger.error(f"Audit trail incomplete: {logged_count}/{expected_count} logged")
  ```

### Long-term Improvements

- [ ] **Consider structured logging library:** JSON logs for machine parsing

  - Use `python-json-logger` for structured output
  - Makes it easier to build monitoring dashboards
  - Better for log aggregation systems

- [ ] **Add transaction log:** Record operations before executing

  - Write-ahead logging pattern
  - Can replay/verify operations
  - Better crash recovery

- [ ] **Add checksums:** Verify file integrity after moves
  - Compute SHA256 before move
  - Verify SHA256 after move
  - Detect corruption during transfer

---

## Production Failure Scenarios

### Scenario 1: Disk Full During Batch Processing

**Current Behavior:**

1. User processes 500 images
2. Disk fills at image #300
3. Progress save fails silently (line 221: `print(f"[!] Error saving progress: {e}")`)
4. File move at #301 fails silently or returns HTTP 500
5. User continues clicking, thinking everything works
6. Session ends, user has no idea which images were processed

**Impact:** Hours of work lost, no way to recover state

**After Fix (DIFF 1, 2, 5):**

1. Progress save failure raises exception
2. Web UI shows clear error: "Disk full - cannot continue"
3. Logs show exact failure point with timestamp
4. User can free disk space and resume from last saved progress

---

### Scenario 2: FileTracker Logging Failure

**Current Behavior:**

1. FileTracker log file becomes corrupted
2. Line 1082-1083: `except Exception: pass  # best-effort logging`
3. All subsequent operations have no audit trail
4. User has no idea logging stopped
5. Need to manually reconcile 300+ files

**Impact:** Complete audit trail loss, manual recovery required

**After Fix (DIFF 1, 2):**

1. FileTracker logging failure logged to stderr
2. Explicit warning: "CRITICAL: Audit trail incomplete!"
3. Lists affected files for manual verification
4. User can decide to continue or stop and fix logging

---

### Scenario 3: Interrupted Session (Ctrl+C)

**Current Behavior:**

1. User presses Ctrl+C during operation
2. Flask server terminates immediately
3. Progress file might be mid-write (JSON corruption)
4. No cleanup, no final save
5. Next run: progress file unreadable, starts from beginning

**Impact:** Session state lost, potential file system inconsistency

**After Fix (DIFF 4):**

1. Signal handler catches Ctrl+C
2. Displays "Shutting down gracefully - please wait"
3. Flask server shuts down cleanly
4. Progress file is in consistent state
5. Next run: resumes from last completed directory

---

### Scenario 4: Silent Similarity Map Corruption

**Current Behavior:**

1. User deletes 50 images from a 500-image directory
2. Line 1025-1026 calls `clean_similarity_maps()` but doesn't check return value
3. Cleaning fails silently (line 628-630: returns False)
4. Similarity map still references deleted images
5. UI layout algorithm uses stale data
6. User sees confusing layout, doesn't match expected similarity grouping

**Impact:** Degraded user experience, wasted time, user loses trust in similarity feature

**After Fix (DIFF 2 + caller improvement):**

1. Similarity map cleaning failure logged with logger.error()
2. Caller checks return value and warns user
3. UI shows banner: "Similarity map out of date - layout may be suboptimal"
4. User can decide to regenerate similarity map or continue

---

## Risk Assessment

### Before Fixes

| Risk Category          | Severity | Likelihood | Impact                    | Mitigation   |
| ---------------------- | -------- | ---------- | ------------------------- | ------------ |
| Silent data loss       | CRITICAL | MEDIUM     | Hours of work lost        | DIFF 1, 2, 5 |
| Audit trail incomplete | HIGH     | HIGH       | Recovery impossible       | DIFF 1, 2    |
| Progress corruption    | MEDIUM   | MEDIUM     | Session restart           | DIFF 2, 4    |
| Undetected failures    | HIGH     | HIGH       | User confusion            | DIFF 2       |
| Crash during operation | MEDIUM   | LOW        | File system inconsistency | DIFF 4       |

### After Fixes

| Risk Category          | Severity | Likelihood | Impact                       | Mitigation Status |
| ---------------------- | -------- | ---------- | ---------------------------- | ----------------- |
| Silent data loss       | LOW      | LOW        | User warned immediately      | ‚úÖ Fixed          |
| Audit trail incomplete | LOW      | LOW        | Explicit warning + file list | ‚úÖ Fixed          |
| Progress corruption    | LOW      | VERY LOW   | Graceful shutdown            | ‚úÖ Fixed          |
| Undetected failures    | LOW      | LOW        | All errors logged            | ‚úÖ Fixed          |
| Crash during operation | LOW      | VERY LOW   | Signal handlers              | ‚úÖ Fixed          |

---

## Summary

**Scripts/03_web_character_sorter.py** is a well-structured web tool with good separation of concerns and comprehensive file operation tracking. However, it has **10 silent exception handlers** that create **critical fragility zones**.

The most dangerous pattern is the **best-effort logging antipattern** (line 1082-1083) which could result in **complete audit trail loss** without any indication to the user. Combined with **no structured logging** and **no graceful shutdown**, the script is vulnerable to **silent failures** that would go undetected until manual inspection.

**The 5 diffs provided address all critical fragility zones:**

1. ‚úÖ Eliminate best-effort antipattern
2. ‚úÖ Add structured logging for debugging
3. ‚úÖ Fix import guards and dead code
4. ‚úÖ Add graceful shutdown handling
5. ‚úÖ Add file operation verification

**After applying these fixes, the script will:**

- Fail-fast instead of silently continuing with corrupt state
- Log all errors with timestamps and stack traces
- Shut down gracefully on Ctrl+C
- Verify all file operations succeeded
- Provide clear error messages to users

**Estimated implementation time:** 2-3 hours  
**Estimated testing time:** 3-4 hours  
**Risk reduction:** HIGH ‚Üí LOW

---

**END OF RAPTOR PHASE A REVIEW**

## Phase B ‚Äì GPT-5 Codex Verification

Prompt file: /Users/eriksjaastad/projects/image-workflow/prompts/raptor_phase_b_codex.md  
[28 tools called]

=== VALIDATION SUMMARY ===

- Diff‚ÄØ1 ‚Äì Orphan `.decision` logging: ‚úÖ Removes the silent `except Exception: pass` and surfaces tracker failures clearly (see current baseline).

```1072:1083:scripts/03_web_character_sorter.py
        if moved:
            try:
                tracker.log_operation(
                    operation="move",
                    source_dir=str(folder.name),
                    dest_dir=str(staging.name),
                    file_count=len(moved),
                    files=moved[:10],
                    notes=f"orphan .decision staged ({len(moved)} total)",
                )
            except Exception:
                pass  # best-effort logging
```

- Diff‚ÄØ2 ‚Äì Structured logging + fail-fast progress saves: ‚úÖ `logging.basicConfig` + `logger.error(..., exc_info=True)` give traceable failures; letting `save_progress` re-raise is consistent with fail-fast goals.
- Diff‚ÄØ3 ‚Äì Import guards & optional dependency logging: ‚úÖ Narrowing to `ImportError` and emitting structured messages keeps behavior while dropping the dead Pillow guard.
- Diff‚ÄØ4 ‚Äì Signal handling: ‚ö†Ô∏è The handler sets `_shutdown_requested` permanently and calls `sys.exit()` on the first signal. The proposed tests rely on module-level state but don‚Äôt reset it, so once `_shutdown_requested` flips, later tests will fail.
- Diff‚ÄØ5 ‚Äì `move_with_metadata` verification: ‚úÖ Pre/post checks align with `move_file_with_all_companions`, giving deterministic failures instead of silent skips.
- Proposed reliability tests: ‚ö†Ô∏è `from scripts.03_web_character_sorter import ...` won‚Äôt parse because module names can‚Äôt start with digits, and the suite never resets `_shutdown_requested`, so the second signal test will observe the ‚Äúforced shutdown‚Äù branch immediately.

=== SUGGESTED ADDITIONS ===

- Load the sorter module in tests via `importlib.util.spec_from_file_location` with an alias, then execute the spec:

```python
spec = importlib.util.spec_from_file_location(
    "scripts._03_web_character_sorter",
    Path(__file__).parent.parent / "scripts" / "03_web_character_sorter.py",
)
character_sorter = importlib.util.module_from_spec(spec)
spec.loader.exec_module(character_sorter)
```

- Before each signal-handling assertion, reset the module state:

```python
monkeypatch.setattr(character_sorter, "_shutdown_requested", False, raising=False)
```

=== TEST RECOMMENDATIONS ===

- Validate that the revised tests fail against the pre-fix code (e.g., missing audit logging) and pass once the diff is applied.
- Add a regression check that `character_sorter.move_with_metadata` raises on duplicate destinations by mocking `shutil.move` to leave the destination missing.
- Consider a smoke test that triggers `tracker.log_operation` failure (e.g., patch `_log_entry` to raise) to confirm the new stderr warnings surface without breaking the request.

=== CONFIDENCE REPORT ===
6/10 ‚Äì Core diffs look sound, but the sketched test suite needs adjustments (import loader + state reset) to run successfully, so I haven‚Äôt seen a green end-to-end run yet.

## Phase C ‚Äì Human Safety Check

Prompt file: /Users/eriksjaastad/projects/image-workflow/prompts/raptor_phase_c_safety.md  
=== MERGE SAFETY REVIEW ===

- High-risk files (name + why)

  - scripts/03_web_character_sorter.py: Silent exception handlers (10), ‚Äúbest-effort‚Äù logging that drops audit trail, unstructured logging, unchecked similarity cleanup, fragile progress saves, missing signal handling, server errors returned to client without server-side logs.

- Subtle issues that automated tools might miss (reasoning)

  - Empty try block around Pillow import indicates dead/accidentally-removed code, masking missing dependency.
  - Optional dependency guards swallow capability loss; users won‚Äôt know HEIC/Trash support is disabled.
  - `clean_similarity_maps(...)` return value is ignored, leaving stale similarity layouts without warning.
  - File operation endpoints return 500 to the browser but capture no server logs/tracebacks, making incident triage hard.
  - Audit trail integrity violation: `except Exception: pass` around `FileTracker.log_operation` can silently nuke traceability mid-session, violating the repo‚Äôs File Safety Rule #7.
  - Module filename starting with a digit complicates imports for tests and tooling; keep tests using file-location import loading (doesn‚Äôt block runtime, but blocks realistic testability unless accounted for).

- Suggested micro-patches or assertions (unified diff or pseudo-code)

  - Micro-patch 1: Remove best-effort logging swallow, emit explicit error and warning
    ```diff
    --- a/scripts/03_web_character_sorter.py
    +++ b/scripts/03_web_character_sorter.py
    @@ -1069,10 +1069,16 @@
         if moved:
    -        try:
    -            tracker.log_operation(..., notes=f"orphan .decision staged ({len(moved)} total)")
    -        except Exception:
    -            pass  # best-effort logging
    +        log_msg = f"orphan .decision staged ({len(moved)} total)"
    +        try:
    +            tracker.log_operation(..., notes=log_msg)
    +        except Exception as e:
    +            print(f"[!!!] CRITICAL: FileTracker logging failed for .decision cleanup: {e}", file=sys.stderr)
    +            print(f"[!!!] Audit trail incomplete! Manual verification required for: {moved}", file=sys.stderr)
    ```
  - Micro-patch 2: Add structured logging and fail-fast progress saves
    ```diff
    --- a/scripts/03_web_character_sorter.py
    +++ b/scripts/03_web_character_sorter.py
    @@ -90,6 +90,16 @@
     +import logging
     +logging.basicConfig(
     +    level=logging.INFO,
     +    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
     +    handlers=[logging.StreamHandler(sys.stdout)],
     +)
     +logger = logging.getLogger(__name__)
     @@ -217,7 +227,8 @@
         try:
             with open(self.progress_file, "w") as f:
                 json.dump(self.session_data, f, indent=2)
         except Exception as e:
    -        print(f"[!] Error saving progress: {e}")
    +        logger.error(f"Failed to save progress to {self.progress_file}: {e}", exc_info=True)
    +        raise
    ```
  - Micro-patch 3: Fix import guards and dead code; surface optional capability status
    ```diff
    --- a/scripts/03_web_character_sorter.py
    +++ b/scripts/03_web_character_sorter.py
    @@ -119,10 +119,13 @@
    -except Exception:
    +except ImportError:
         print("[!] Flask is required. Install with: pip install flask", file=sys.stderr)
    +    logger.critical("Flask import failed - cannot run web interface", exc_info=True)
         raise
    -try:
    -    pass
    -except Exception:
    -    print("[!] Pillow is required. Install with: pip install pillow", file=sys.stderr)
    -    raise
    +try:
    +    from pillow_heif import register_heif_opener  # type: ignore
    +    register_heif_opener()
    +    logger.info("HEIC/HEIF support enabled via pillow-heif")
    +except ImportError:
    +    logger.info("pillow-heif not available - HEIC/HEIF files will not be supported")
    +except Exception as e:
    +    logger.warning(f"pillow-heif registration failed: {e} - HEIC/HEIF support disabled")
     _SEND2TRASH_AVAILABLE = False
     try:
    -    # (existing)
    -except Exception:
    -    _SEND2TRASH_AVAILABLE = False
    +    from send2trash import send2trash
    +    _SEND2TRASH_AVAILABLE = True
    +    logger.info("send2trash available - safe deletion enabled")
    +except ImportError:
    +    _SEND2TRASH_AVAILABLE = False
    +    logger.warning("send2trash not available - will use hard delete behavior")
    ```
  - Micro-patch 4: Register graceful shutdown handlers
    ```diff
    --- a/scripts/03_web_character_sorter.py
    +++ b/scripts/03_web_character_sorter.py
    @@ -140,6 +140,22 @@
    +_shutdown_requested = False
    +def _signal_handler(signum, frame):
    +    global _shutdown_requested
    +    if _shutdown_requested:
    +        logger.warning("Forced shutdown - progress may be lost")
    +        sys.exit(1)
    +    _shutdown_requested = True
    +    logger.info(f"Received {signal.Signals(signum).name} - shutting down gracefully...")
    +    print("\n[*] Shutting down gracefully - please wait for cleanup...")
    +    print("[*] Shutdown complete. Progress has been saved.")
    +    sys.exit(0)
    +
    +signal.signal(signal.SIGINT, _signal_handler)
    +signal.signal(signal.SIGTERM, _signal_handler)
    ```
  - Micro-patch 5: Verify file operations and check similarity cleanup result
    ```diff
    --- a/scripts/03_web_character_sorter.py
    +++ b/scripts/03_web_character_sorter.py
    @@ -530,6 +530,20 @@ def move_with_metadata(src_path, dest_dir, tracker, group_name):
    +    if not src_path.exists():
    +        msg = f"Source file does not exist: {src_path}"
    +        logger.error(msg)
    +        raise FileNotFoundError(msg)
         moved_files = move_file_with_all_companions(src_path, dest_dir, dry_run=False)
    +    if not moved_files:
    +        msg = f"No files moved for {src_path.name} - operation may have failed silently"
    +        logger.error(msg)
    +        raise RuntimeError(msg)
    +    dest_file = dest_dir / src_path.name
    +    if not dest_file.exists():
    +        msg = f"Move reported success but destination file missing: {dest_file}"
    +        logger.error(msg)
    +        raise RuntimeError(msg)
    @@ -1023,6 +1035,10 @@
         if similarity_map_dir:
             ok = clean_similarity_maps(app.config["CURRENT_DIR"], similarity_map_dir)
    +        if not ok:
    +            logger.error("Similarity map cleanup failed; layout may be stale")
    +            # Optionally surface a UI banner on the next response
    ```

- Final verdict: ‚ùå Block Merge

- Confidence score (0‚Äì10)

  - 7 ‚Äî Findings are consistent with Phase A/B. The diffs are minimal, targeted, and align with repo safety rules; actual line offsets may vary slightly.

- Short code reference (current baseline showing the critical antipattern)

```1072:1083:scripts/03_web_character_sorter.py
        if moved:
            try:
                tracker.log_operation(
                    operation="move",
                    source_dir=str(folder.name),
                    dest_dir=str(staging.name),
                    file_count=len(moved),
                    files=moved[:10],
                    notes=f"orphan .decision staged ({len(moved)} total)",
                )
            except Exception:
                pass  # best-effort logging
```

Status: I reviewed the Phase A/B outputs and focused the final gate on eliminating silent failure modes and ensuring auditability; the above micro-patches are the minimum required to make the merge safe.

- Replace the silent `except Exception: pass` around `FileTracker` logging; add structured logging and fail-fast progress saves; correct import guards; add signal handlers; verify file ops and check similarity cleanup results.
